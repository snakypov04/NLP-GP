"""
=============================================================================
Comparative Visualization Module for Financial Sentiment Analysis
=============================================================================

Publication-quality visualizations comparing Phase 1 Baseline (LR) vs BiLSTM
Optimized for Google Colab display system

Features:
- Colab-optimized matplotlib backend and display
- Interactive widgets for parameter tuning
- Comparative performance visualizations
- BiLSTM-specific training visualizations
- Memory management for large datasets
- Export functionality (PNG/PDF)

Author: NLP Engineering Team
Date: 2024
=============================================================================
"""

import os
import json
import gc
import warnings
from pathlib import Path
from typing import Dict, Tuple, Optional, List
import logging

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
from scipy import stats

# Colab-specific imports with fallbacks
try:
    from IPython.display import display, HTML, clear_output
    import ipywidgets as widgets
    from ipywidgets import interact, interactive, fixed, interact_manual
    COLAB_AVAILABLE = True
except ImportError:
    COLAB_AVAILABLE = False
    print("Note: Running outside Colab - interactive widgets disabled")

# Memory monitoring
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =============================================================================
# COLAB DISPLAY OPTIMIZATIONS
# =============================================================================


def configure_colab_display():
    """Configure matplotlib for optimal Colab display."""
    # Set backend for Colab
    if COLAB_AVAILABLE:
        matplotlib.use('Agg')  # Use non-interactive backend
        plt.ioff()  # Turn off interactive mode

    # High-DPI rendering for sharp images
    plt.rcParams['figure.dpi'] = 100
    plt.rcParams['savefig.dpi'] = 300
    plt.rcParams['savefig.bbox'] = 'tight'
    plt.rcParams['savefig.pad_inches'] = 0.1

    # Colab-optimized figure sizes
    plt.rcParams['figure.figsize'] = (14, 8)
    plt.rcParams['figure.max_open_warning'] = 0

    # Professional styling
    sns.set_style("whitegrid")
    plt.rcParams['font.size'] = 11
    plt.rcParams['axes.labelsize'] = 12
    plt.rcParams['axes.titlesize'] = 14
    plt.rcParams['xtick.labelsize'] = 10
    plt.rcParams['ytick.labelsize'] = 10
    plt.rcParams['legend.fontsize'] = 10
    plt.rcParams['figure.titlesize'] = 16

    # Finance-appropriate color scheme
    plt.rcParams['axes.prop_cycle'] = plt.cycler(
        color=['#1f77b4', '#2ca02c', '#d62728', '#ff7f0e', '#9467bd']
    )

    logger.info("‚úì Colab display configured")


def auto_display_plot(fig, save_path: Optional[str] = None,
                      show_in_colab: bool = True):
    """
    Automatically display plot in Colab and save if path provided.

    Args:
        fig: Matplotlib figure object
        save_path: Optional path to save figure
        show_in_colab: Whether to display in Colab
    """
    if save_path:
        fig.savefig(save_path, dpi=300, bbox_inches='tight',
                    facecolor='white', edgecolor='none')
        logger.info(f"‚úì Saved: {save_path}")

    if COLAB_AVAILABLE and show_in_colab:
        display(fig)

    plt.close(fig)
    gc.collect()


# =============================================================================
# MEMORY MANAGEMENT
# =============================================================================

def get_memory_usage() -> Dict:
    """Get current memory usage statistics."""
    if not PSUTIL_AVAILABLE:
        return {'available': True}

    mem = psutil.virtual_memory()
    return {
        'total_gb': mem.total / (1024**3),
        'used_gb': mem.used / (1024**3),
        'available_gb': mem.available / (1024**3),
        'percent': mem.percent
    }


def check_memory_warning(threshold: float = 85.0) -> bool:
    """
    Check if memory usage exceeds threshold and warn.

    Args:
        threshold: Memory usage percentage threshold

    Returns:
        True if warning should be issued
    """
    if not PSUTIL_AVAILABLE:
        return False

    mem = get_memory_usage()
    if mem['percent'] > threshold:
        logger.warning(
            f"‚ö†Ô∏è  Memory usage at {mem['percent']:.1f}% - "
            f"consider clearing variables"
        )
        return True
    return False


def clear_memory():
    """Clear memory and run garbage collection."""
    gc.collect()
    if COLAB_AVAILABLE:
        try:
            from IPython import get_ipython
            get_ipython().magic('reset -sf')
        except:
            pass
    logger.info("‚úì Memory cleared")


# =============================================================================
# DATA LOADING
# =============================================================================

def load_metrics(baseline_path: str, bilstm_path: str) -> Tuple[Dict, Dict]:
    """
    Load metrics from both models.

    Args:
        baseline_path: Path to baseline metrics JSON
        bilstm_path: Path to BiLSTM metrics JSON

    Returns:
        Tuple of (baseline_metrics, bilstm_metrics)
    """
    logger.info("Loading metrics...")

    with open(baseline_path, 'r') as f:
        baseline_data = json.load(f)

    with open(bilstm_path, 'r') as f:
        bilstm_data = json.load(f)

    # Extract test metrics (baseline has nested structure)
    baseline_metrics = baseline_data.get('test_metrics', baseline_data)
    bilstm_metrics = bilstm_data

    logger.info("‚úì Metrics loaded")
    return baseline_metrics, bilstm_metrics


def load_training_history(history_path: Optional[str] = None) -> Optional[Dict]:
    """
    Load training history if available.

    Args:
        history_path: Path to training history JSON (optional)

    Returns:
        Training history dict or None
    """
    if history_path and os.path.exists(history_path):
        with open(history_path, 'r') as f:
            return json.load(f)
    return None


# =============================================================================
# COMPARATIVE VISUALIZATIONS
# =============================================================================

def plot_performance_comparison(
    baseline_metrics: Dict,
    bilstm_metrics: Dict,
    save_path: Optional[str] = None
):
    """
    Create side-by-side performance comparison bar chart.

    Args:
        baseline_metrics: Baseline model metrics
        bilstm_metrics: BiLSTM model metrics
        save_path: Optional path to save figure
    """
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Model Performance Comparison: Baseline LR vs BiLSTM',
                 fontsize=16, fontweight='bold', y=0.995)

    # Extract metrics
    baseline_cv = baseline_metrics.get('cross_validation', {})
    baseline_std = baseline_cv.get('std_accuracy', 0.0)

    models = ['Baseline LR', 'BiLSTM']
    colors = ['#1f77b4', '#2ca02c']

    # 1. Overall Accuracy
    ax = axes[0, 0]
    accuracies = [
        baseline_metrics['accuracy'],
        bilstm_metrics['accuracy']
    ]
    errors = [baseline_std, 0.0]  # BiLSTM doesn't have CV std

    bars = ax.bar(models, accuracies, yerr=errors, capsize=10,
                  color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
    ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')
    ax.set_title('Overall Accuracy', fontsize=13, fontweight='bold')
    ax.set_ylim([0.65, 0.85])
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # Add value labels on bars
    for i, (bar, acc) in enumerate(zip(bars, accuracies)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')

    # Add improvement annotation
    improvement = accuracies[1] - accuracies[0]
    ax.annotate(f'+{improvement:.3f}',
                xy=(1, accuracies[1]), xytext=(1.3, accuracies[1] + 0.02),
                arrowprops=dict(arrowstyle='->', color='green', lw=2),
                fontsize=12, fontweight='bold', color='green')

    # 2. Weighted F1 Score
    ax = axes[0, 1]
    f1_scores = [
        baseline_metrics['weighted_f1'],
        bilstm_metrics['weighted_f1']
    ]

    bars = ax.bar(models, f1_scores, color=colors, alpha=0.8,
                  edgecolor='black', linewidth=1.5)
    ax.set_ylabel('Weighted F1 Score', fontsize=12, fontweight='bold')
    ax.set_title('Weighted F1 Score', fontsize=13, fontweight='bold')
    ax.set_ylim([0.65, 0.85])
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    for i, (bar, f1) in enumerate(zip(bars, f1_scores)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')

    improvement = f1_scores[1] - f1_scores[0]
    ax.annotate(f'+{improvement:.3f}',
                xy=(1, f1_scores[1]), xytext=(1.3, f1_scores[1] + 0.02),
                arrowprops=dict(arrowstyle='->', color='green', lw=2),
                fontsize=12, fontweight='bold', color='green')

    # 3. Per-Class F1 Scores
    ax = axes[1, 0]
    classes = ['Negative', 'Neutral', 'Positive']
    x = np.arange(len(classes))
    width = 0.35

    baseline_f1 = [
        baseline_metrics['per_class_metrics']['negative']['f1'],
        baseline_metrics['per_class_metrics']['neutral']['f1'],
        baseline_metrics['per_class_metrics']['positive']['f1']
    ]
    bilstm_f1 = [
        bilstm_metrics['per_class_metrics']['negative']['f1'],
        bilstm_metrics['per_class_metrics']['neutral']['f1'],
        bilstm_metrics['per_class_metrics']['positive']['f1']
    ]

    bars1 = ax.bar(x - width/2, baseline_f1, width, label='Baseline LR',
                   color=colors[0], alpha=0.8, edgecolor='black', linewidth=1.5)
    bars2 = ax.bar(x + width/2, bilstm_f1, width, label='BiLSTM',
                   color=colors[1], alpha=0.8, edgecolor='black', linewidth=1.5)

    ax.set_ylabel('F1 Score', fontsize=12, fontweight='bold')
    ax.set_title('Per-Class F1 Scores', fontsize=13, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(classes)
    ax.legend()
    ax.set_ylim([0.65, 0.85])
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # Add value labels
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)

    # 4. ROC AUC Scores
    ax = axes[1, 1]
    baseline_auc = [
        baseline_metrics['roc_auc_scores']['class_-1'],
        baseline_metrics['roc_auc_scores']['class_0'],
        baseline_metrics['roc_auc_scores']['class_1']
    ]
    bilstm_auc = [
        bilstm_metrics['roc_auc_scores']['class_-1'],
        bilstm_metrics['roc_auc_scores']['class_0'],
        bilstm_metrics['roc_auc_scores']['class_1']
    ]

    bars1 = ax.bar(x - width/2, baseline_auc, width, label='Baseline LR',
                   color=colors[0], alpha=0.8, edgecolor='black', linewidth=1.5)
    bars2 = ax.bar(x + width/2, bilstm_auc, width, label='BiLSTM',
                   color=colors[1], alpha=0.8, edgecolor='black', linewidth=1.5)

    ax.set_ylabel('ROC AUC Score', fontsize=12, fontweight='bold')
    ax.set_title('ROC AUC Scores (One-vs-Rest)',
                 fontsize=13, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(classes)
    ax.legend()
    ax.set_ylim([0.80, 0.95])
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # Add value labels
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    auto_display_plot(fig, save_path)


def plot_roc_comparison(
    baseline_metrics: Dict,
    bilstm_metrics: Dict,
    save_path: Optional[str] = None
):
    """
    Create overlaid ROC curve comparison.

    Args:
        baseline_metrics: Baseline model metrics
        bilstm_metrics: BiLSTM model metrics
        save_path: Optional path to save figure
    """
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle('ROC Curve Comparison: Baseline LR vs BiLSTM',
                 fontsize=16, fontweight='bold')

    classes = ['Negative', 'Neutral', 'Positive']
    class_values = [-1, 0, 1]
    colors_class = ['#d62728', '#1f77b4', '#2ca02c']

    # Load ROC curves if available (would need to be saved separately)
    # For now, we'll create theoretical curves based on AUC scores

    for idx, (class_name, class_val, color) in enumerate(
        zip(classes, class_values, colors_class)
    ):
        ax = axes[idx]

        # Get AUC scores
        baseline_auc = baseline_metrics['roc_auc_scores'][f'class_{class_val}']
        bilstm_auc = bilstm_metrics['roc_auc_scores'][f'class_{class_val}']

        # Create theoretical ROC curves (simplified)
        fpr = np.linspace(0, 1, 100)

        # Approximate TPR from AUC (simplified model)
        def auc_to_tpr(fpr, auc_score):
            # Simple approximation: TPR = AUC * FPR + (1-AUC) * FPR^2
            return auc_score * fpr + (1 - auc_score) * fpr ** 2

        baseline_tpr = auc_to_tpr(fpr, baseline_auc)
        bilstm_tpr = auc_to_tpr(fpr, bilstm_auc)

        # Plot curves
        ax.plot(fpr, baseline_tpr, '--', linewidth=2.5,
                label=f'Baseline LR (AUC = {baseline_auc:.3f})',
                color='#1f77b4', alpha=0.8)
        ax.plot(fpr, bilstm_tpr, '-', linewidth=2.5,
                label=f'BiLSTM (AUC = {bilstm_auc:.3f})',
                color='#2ca02c', alpha=0.8)
        ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5,
                label='Random Classifier')

        ax.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')
        ax.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')
        ax.set_title(f'{class_name} Class', fontsize=12, fontweight='bold')
        ax.legend(loc='lower right', fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])

        # Add improvement annotation
        improvement = bilstm_auc - baseline_auc
        ax.text(0.6, 0.2, f'ŒîAUC: +{improvement:.3f}',
                fontsize=10, fontweight='bold', color='green',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()
    auto_display_plot(fig, save_path)


def plot_confusion_matrix_comparison(
    baseline_metrics: Dict,
    bilstm_metrics: Dict,
    save_path: Optional[str] = None
):
    """
    Create side-by-side confusion matrix comparison with normalized percentages.

    Args:
        baseline_metrics: Baseline model metrics
        bilstm_metrics: BiLSTM model metrics
        save_path: Optional path to save figure
    """
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    fig.suptitle('Confusion Matrix Comparison (Normalized Percentages)',
                 fontsize=16, fontweight='bold')

    labels = ['Negative', 'Neutral', 'Positive']

    for idx, (metrics, title) in enumerate([
        (baseline_metrics, 'Baseline LR'),
        (bilstm_metrics, 'BiLSTM')
    ]):
        ax = axes[idx]
        cm = np.array(metrics['confusion_matrix'])

        # Normalize to percentages
        cm_normalized = (cm / cm.sum(axis=1, keepdims=True)) * 100

        # Create heatmap
        sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Blues',
                    xticklabels=labels, yticklabels=labels,
                    cbar_kws={'label': 'Percentage (%)'}, ax=ax,
                    vmin=0, vmax=100, linewidths=1, linecolor='black')

        ax.set_title(f'{title}\n(Accuracy: {metrics["accuracy"]:.3f})',
                     fontsize=13, fontweight='bold')
        ax.set_ylabel('True Label', fontsize=11, fontweight='bold')
        ax.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')

    plt.tight_layout()
    auto_display_plot(fig, save_path)


# =============================================================================
# BILSTM-SPECIFIC VISUALIZATIONS
# =============================================================================

def plot_training_curves_interactive(
    history: Dict,
    save_path: Optional[str] = None,
    epoch_range: Optional[Tuple[int, int]] = None
):
    """
    Plot BiLSTM training/validation curves with optional epoch range.

    Args:
        history: Training history dictionary
        save_path: Optional path to save figure
        epoch_range: Optional tuple of (start_epoch, end_epoch)
    """
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    fig.suptitle('BiLSTM Training History', fontsize=16, fontweight='bold')

    if epoch_range:
        start, end = epoch_range
        epochs = list(range(start, min(end, len(history['loss']))))
    else:
        epochs = list(range(len(history['loss'])))

    # Loss
    ax = axes[0, 0]
    ax.plot(epochs, [history['loss'][e] for e in epochs],
            label='Training Loss', linewidth=2.5, color='#1f77b4')
    ax.plot(epochs, [history['val_loss'][e] for e in epochs],
            label='Validation Loss', linewidth=2.5, color='#d62728', linestyle='--')
    ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')
    ax.set_ylabel('Loss', fontsize=11, fontweight='bold')
    ax.set_title('Model Loss', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Accuracy
    ax = axes[0, 1]
    ax.plot(epochs, [history['accuracy'][e] for e in epochs],
            label='Training Accuracy', linewidth=2.5, color='#2ca02c')
    ax.plot(epochs, [history['val_accuracy'][e] for e in epochs],
            label='Validation Accuracy', linewidth=2.5, color='#ff7f0e', linestyle='--')
    ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')
    ax.set_ylabel('Accuracy', fontsize=11, fontweight='bold')
    ax.set_title('Model Accuracy', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Learning rate (if available)
    ax = axes[1, 0]
    if 'lr' in history:
        ax.plot(epochs, [history['lr'][e] for e in epochs],
                linewidth=2.5, color='#9467bd')
        ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')
        ax.set_ylabel('Learning Rate', fontsize=11, fontweight='bold')
        ax.set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')
        ax.set_yscale('log')
    else:
        ax.text(0.5, 0.5, 'Learning rate data not available',
                ha='center', va='center', transform=ax.transAxes)
        ax.set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)

    # Overfitting analysis
    ax = axes[1, 1]
    train_acc = [history['accuracy'][e] for e in epochs]
    val_acc = [history['val_accuracy'][e] for e in epochs]
    gap = [t - v for t, v in zip(train_acc, val_acc)]

    ax.plot(epochs, gap, linewidth=2.5, color='#d62728', label='Train-Val Gap')
    ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)
    ax.fill_between(epochs, 0, gap, alpha=0.3, color='#d62728')
    ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')
    ax.set_ylabel('Accuracy Gap', fontsize=11, fontweight='bold')
    ax.set_title('Overfitting Analysis (Train - Val Accuracy)',
                 fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    auto_display_plot(fig, save_path)


# =============================================================================
# INTERACTIVE WIDGETS (Colab-specific)
# =============================================================================

def create_interactive_dashboard(
    baseline_metrics: Dict,
    bilstm_metrics: Dict,
    history: Optional[Dict] = None
):
    """
    Create interactive dashboard with widgets (Colab only).

    Args:
        baseline_metrics: Baseline model metrics
        bilstm_metrics: BiLSTM model metrics
        history: Optional training history
    """
    if not COLAB_AVAILABLE:
        logger.warning("Interactive dashboard requires Colab environment")
        return

    def update_visualization(metric_type='accuracy', class_filter='all'):
        """Update visualization based on widget selection."""
        clear_output(wait=True)

        if metric_type == 'accuracy':
            plot_performance_comparison(baseline_metrics, bilstm_metrics)
        elif metric_type == 'roc':
            plot_roc_comparison(baseline_metrics, bilstm_metrics)
        elif metric_type == 'confusion':
            plot_confusion_matrix_comparison(baseline_metrics, bilstm_metrics)
        elif metric_type == 'training' and history:
            plot_training_curves_interactive(history)

    # Create widgets
    metric_widget = widgets.Dropdown(
        options=['accuracy', 'roc', 'confusion', 'training'],
        value='accuracy',
        description='Metric:',
        style={'description_width': 'initial'}
    )

    class_widget = widgets.Dropdown(
        options=['all', 'negative', 'neutral', 'positive'],
        value='all',
        description='Class:',
        style={'description_width': 'initial'}
    )

    # Create interactive interface
    interact(update_visualization,
             metric_type=metric_widget,
             class_filter=class_widget)

    logger.info("‚úì Interactive dashboard created")


# =============================================================================
# EXPORT FUNCTIONALITY
# =============================================================================

def export_all_visualizations(
    baseline_metrics: Dict,
    bilstm_metrics: Dict,
    history: Optional[Dict] = None,
    output_dir: str = 'comparison_results'
):
    """
    Export all visualizations to PNG and PDF.

    Args:
        baseline_metrics: Baseline model metrics
        bilstm_metrics: BiLSTM model metrics
        history: Optional training history
        output_dir: Output directory path
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    logger.info(f"Exporting visualizations to {output_dir}...")

    # Create individual PNG files
    plot_performance_comparison(
        baseline_metrics, bilstm_metrics,
        save_path=str(output_path / 'performance_comparison.png')
    )

    plot_roc_comparison(
        baseline_metrics, bilstm_metrics,
        save_path=str(output_path / 'roc_comparison.png')
    )

    plot_confusion_matrix_comparison(
        baseline_metrics, bilstm_metrics,
        save_path=str(output_path / 'confusion_matrix_comparison.png')
    )

    if history:
        plot_training_curves_interactive(
            history,
            save_path=str(output_path / 'training_curves.png')
        )

    # Create combined PDF report
    pdf_path = output_path / 'comparison_report.pdf'
    with PdfPages(pdf_path) as pdf:
        # Performance comparison
        fig, _ = plt.subplots(figsize=(16, 12))
        plot_performance_comparison(baseline_metrics, bilstm_metrics)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # ROC comparison
        fig, _ = plt.subplots(figsize=(18, 5))
        plot_roc_comparison(baseline_metrics, bilstm_metrics)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        # Confusion matrix
        fig, _ = plt.subplots(figsize=(16, 6))
        plot_confusion_matrix_comparison(baseline_metrics, bilstm_metrics)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close(fig)

        if history:
            fig, _ = plt.subplots(figsize=(16, 10))
            plot_training_curves_interactive(history)
            pdf.savefig(fig, bbox_inches='tight')
            plt.close(fig)

    logger.info(f"‚úì All visualizations exported to {output_dir}")
    logger.info(f"‚úì PDF report saved: {pdf_path}")


# =============================================================================
# MAIN FUNCTION
# =============================================================================

def main():
    """Main function to generate all comparative visualizations."""
    logger.info("=" * 80)
    logger.info("COMPARATIVE VISUALIZATION MODULE")
    logger.info("=" * 80)

    # Configure Colab display
    configure_colab_display()

    # Check memory
    check_memory_warning()

    # Paths
    baseline_path = 'baseline_results/baseline_metrics.json'
    bilstm_path = 'bilstm_results/bilstm_metrics.json'
    output_dir = 'comparison_results'

    # Load metrics
    baseline_metrics, bilstm_metrics = load_metrics(baseline_path, bilstm_path)

    # Load training history if available
    history = None
    history_path = 'bilstm_results/training_history.json'
    if os.path.exists(history_path):
        history = load_training_history(history_path)

    # Generate all visualizations
    logger.info("\nüìä Generating comparative visualizations...")

    plot_performance_comparison(baseline_metrics, bilstm_metrics)
    plot_roc_comparison(baseline_metrics, bilstm_metrics)
    plot_confusion_matrix_comparison(baseline_metrics, bilstm_metrics)

    if history:
        plot_training_curves_interactive(history)

    # Export all visualizations
    export_all_visualizations(
        baseline_metrics, bilstm_metrics, history, output_dir
    )

    # Create interactive dashboard (Colab only)
    if COLAB_AVAILABLE:
        logger.info("\nüéõÔ∏è  Creating interactive dashboard...")
        create_interactive_dashboard(baseline_metrics, bilstm_metrics, history)

    logger.info("=" * 80)
    logger.info("‚úì All visualizations generated successfully!")
    logger.info("=" * 80)

    # Final memory check
    check_memory_warning()


if __name__ == "__main__":
    main()
